{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh6WTxiVYqkh"
      },
      "outputs": [],
      "source": [
        "!pip install bs_ds\n",
        "!pip install fake_useragent\n",
        "!pip install lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaWIHHicwtAr"
      },
      "source": [
        "## Using python's `requests` module:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JpBMzWysPb0"
      },
      "source": [
        "\n",
        "-  Use `requests` library to initiate connections to a website.\n",
        "- Check the status code returned to determine if connection was successful (status code=200)\n",
        "\n",
        "```python\n",
        "import requests\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "\n",
        "# Connect to the url using requests.get\n",
        "response = requests.get(url)\n",
        "response.status_code\n",
        "```\n",
        "\n",
        " ___\n",
        "| Status Code | Code Meaning\n",
        "| --------- | -------------|\n",
        "1xx |   Informational\n",
        "2xx|    Success\n",
        "3xx|     Redirection\n",
        "4xx|     Client Error\n",
        "5xx |    Server Error\n",
        "\n",
        "___\n",
        "- **Note: You can add a `timeout` to `requests.get()` to avoid indefinite waiting**\n",
        "    - Best in multiples of 3 (`timeout=3` or `6` , `9` ,etc.)\n",
        "\n",
        "```python\n",
        "# Add a timeout to prevent hanging\n",
        "response = requests.get(url, timeout=3)\n",
        "response.status_code\n",
        "\n",
        "```\n",
        "- **`response` is a dictionary with the contents printed below**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "id": "w5Rn-Y0N0q3B",
        "outputId": "eb7b1af9-2a9a-447a-cb7b-afcd1ce72ba0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Status code:  200\n",
            "Connection successfull.\n",
            "\n",
            "\n",
            "------------------------------------------------------------\n",
            "\tContents of Response.items():\n",
            "------------------------------------------------------------\n",
            "Date                     : Mon, 24 Jun 2019 15:13:12 GMT           \n",
            "Content-Type             : text/html; charset=UTF-8                \n",
            "Content-Length           : 64418                                   \n",
            "Connection               : keep-alive                              \n",
            "Server                   : mw1274.eqiad.wmnet                      \n",
            "X-Content-Type-Options   : nosniff                                 \n",
            "P3P                      : CP=\"This is not a P3P policy! See https://en.wikipedia.org/wiki/Special:CentralAutoLogin/P3P for more info.\"\n",
            "X-Powered-By             : HHVM/3.18.6-dev                         \n",
            "Content-language         : en                                      \n",
            "Last-Modified            : Mon, 24 Jun 2019 13:58:29 GMT           \n",
            "Backend-Timing           : D=137942 t=1561384720599492             \n",
            "Vary                     : Accept-Encoding,Cookie,Authorization,X-Seven\n",
            "Content-Encoding         : gzip                                    \n",
            "X-Varnish                : 458249968 437952989, 205788053 193766102\n",
            "Via                      : 1.1 varnish (Varnish/5.1), 1.1 varnish (Varnish/5.1)\n",
            "Age                      : 4471                                    \n",
            "X-Cache                  : cp1089 hit/10, cp1083 hit/1             \n",
            "X-Cache-Status           : hit-front                               \n",
            "Server-Timing            : cache;desc=\"hit-front\"                  \n",
            "Strict-Transport-Security: max-age=106384710; includeSubDomains; preload\n",
            "Set-Cookie               : WMF-Last-Access=24-Jun-2019;Path=/;HttpOnly;secure;Expires=Fri, 26 Jul 2019 12:00:00 GMT, WMF-Last-Access-Global=24-Jun-2019;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Fri, 26 Jul 2019 12:00:00 GMT, GeoIP=US:VA::38.66:-77.25:v4; Path=/; secure; Domain=.wikipedia.org\n",
            "X-Analytics              : ns=0;page_id=52328;https=1;nocookies=1  \n",
            "X-Client-IP              : 35.237.177.107                          \n",
            "Cache-Control            : private, s-maxage=0, max-age=0, must-revalidate\n",
            "Accept-Ranges            : bytes                                   \n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "\n",
        "response = requests.get(url, timeout=3)\n",
        "print('Status code: ',response.status_code)\n",
        "if response.status_code==200:\n",
        "    print('Connection successfull.\\n\\n')\n",
        "else:\n",
        "    print('Error. Check status code table.\\n\\n')\n",
        "\n",
        "\n",
        "\n",
        "# Print out the contents of a request's response\n",
        "print(f\"{'---'*20}\\n\\tContents of Response.items():\\n{'---'*20}\")\n",
        "\n",
        "for k,v in response.headers.items():\n",
        "    print(f\"{k:{25}}: {v:{40}}\") # Note: add :{number} inside of a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "fbpLnJrzDetE",
        "outputId": "c1b0484f-4902-4114-ea8a-74d83623a5e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Date: Mon, 24 Jun 2019 15:13:12 GMT\n",
            "Content-Type: text/html; charset=UTF-8\n",
            "Content-Length: 64418\n",
            "Connection: keep-alive\n",
            "Server: mw1274.eqiad.wmnet\n",
            "X-Content-Type-Options: nosniff\n",
            "P3P: CP=\"This is not a P3P policy! See https://en.wikipedia.org/wiki/Special:CentralAutoLogin/P3P for more info.\"\n",
            "X-Powered-By: HHVM/3.18.6-dev\n",
            "Content-language: en\n",
            "Last-Modified: Mon, 24 Jun 2019 13:58:29 GMT\n",
            "Backend-Timing: D=137942 t=1561384720599492\n",
            "Vary: Accept-Encoding,Cookie,Authorization,X-Seven\n",
            "Content-Encoding: gzip\n",
            "X-Varnish: 458249968 437952989, 205788053 193766102\n",
            "Via: 1.1 varnish (Varnish/5.1), 1.1 varnish (Varnish/5.1)\n",
            "Age: 4471\n",
            "X-Cache: cp1089 hit/10, cp1083 hit/1\n",
            "X-Cache-Status: hit-front\n",
            "Server-Timing: cache;desc=\"hit-front\"\n",
            "Strict-Transport-Security: max-age=106384710; includeSubDomains; preload\n",
            "Set-Cookie: WMF-Last-Access=24-Jun-2019;Path=/;HttpOnly;secure;Expires=Fri, 26 Jul 2019 12:00:00 GMT, WMF-Last-Access-Global=24-Jun-2019;Path=/;Domain=.wikipedia.org;HttpOnly;secure;Expires=Fri, 26 Jul 2019 12:00:00 GMT, GeoIP=US:VA::38.66:-77.25:v4; Path=/; secure; Domain=.wikipedia.org\n",
            "X-Analytics: ns=0;page_id=52328;https=1;nocookies=1\n",
            "X-Client-IP: 35.237.177.107\n",
            "Cache-Control: private, s-maxage=0, max-age=0, must-revalidate\n",
            "Accept-Ranges: bytes\n"
          ]
        }
      ],
      "source": [
        "for k,v in response.headers.items():\n",
        "    print(f\"{k}: {v}\") # Note: add :{number} inside of a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAPbGwxTDZbG"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjXVI4Qpw2_n"
      },
      "source": [
        "## Random Tips - Text Printing/Formatting:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkwN3WP80tiS"
      },
      "source": [
        "\n",
        "- **You can repeat strings by using multiplication**\n",
        "    - `'---'*20` will repeat the dashed lines 20 times\n",
        "\n",
        "- **You can determine how much space is alloted for a variable when using f-strings**\n",
        "    - Add a `:{##}` after the variable to specify the allocated width\n",
        "    - Add a `>` before the `{##}` to force alignment\n",
        "    - Add another symbol (like '.'' or '-') before `>` to add guiding-line/placeholder (like in a table of contents)\n",
        "\n",
        "```python\n",
        "print(f\"Status code: {response.status_code}\")\n",
        "print(f\"Status code: {response.status_code:>{20}}\")\n",
        "print(f\"Status code: {response.status_code:->{20}}\")\n",
        "```    \n",
        "```\n",
        "# Returns:\n",
        "Status code: 200\n",
        "Status code:                  200\n",
        "Status code: -----------------200\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hO3ac1hE8gr5"
      },
      "source": [
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xSwUhVVTwvRq"
      },
      "source": [
        "## Quick Review -  HTML & Tags\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jZuk4QQ17nU"
      },
      "source": [
        "- All HTML pages have the following components\n",
        "    1. document declaration followed by html tag\n",
        "    \n",
        "    `<!DOCTYPE html>`<br>\n",
        "    `<html>`\n",
        "    2. Head\n",
        "     html tag<br>\n",
        "    `<head> <title></title></head>`\n",
        "    3. Body<br>\n",
        "    `<body>` ... content... `</body>`<br>\n",
        "    `</html>`\n",
        "\n",
        "- Html content is divdied into **tags** that specify the type of content.\n",
        "    - [Basic Tags Reference Table](https://www.w3schools.com/tags/ref_byfunc.asp)\n",
        "    - [Full Alphabetical Tag Reference Table](https://www.w3schools.com/tags/)\n",
        "    \n",
        "    - **tags** have attributes\n",
        "        - [Tag Attributes](https://www.w3schools.com/html/html_attributes.asp)\n",
        "        - Attributes are always defined in the start/opening tag.\n",
        "\n",
        "    - **tags** may have several content-creator-defined attributes such as `class` or `id`\n",
        "- We will **use the tag and its identifying attributes to isolate content** we want on a web page with BeautifulSoup.\n",
        "\n",
        "___\n",
        "___"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7H_4da2vkGL"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we-zGs8lw7kY"
      },
      "source": [
        "#  1) Using `BeautifulSoup`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDQT-D171lhn"
      },
      "source": [
        "\n",
        "## Cook a soup\n",
        "\n",
        "- Connect to a website using`response = requests.get(url)`\n",
        "- Feed `response.content` into BeautifulSoup\n",
        "- Must specify the parser that will analyze the contents\n",
        "    - default available is `'html.parser'`\n",
        "    - recommended is to install and use `lxml` [[lxml documentation](https://lxml.de/3.7/)]\n",
        "- use soup.prettify() to get a user-friendly version of the content to print\n",
        "\n",
        "```python\n",
        "# Define Url and establish connection\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "response = requests.get(url, timeout=3)\n",
        "\n",
        "# Feed the response's .content into BeauitfulSoup\n",
        "page_content = response.content\n",
        "soup = BeautifulSoup(page_content,'lxml') #'html.parser')\n",
        "\n",
        "# Preview soup contents using .prettify()\n",
        "print(soup.prettify()[:2000])\n",
        "\n",
        "```\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FieLZ63VVEXi"
      },
      "source": [
        "## What's in a Soup?\n",
        "- **A soup is essentially a collection of `tag objects`**\n",
        "    - each tag from the html is a tag object in the soup\n",
        "    - the tag's maintain the hierarchy of the html page, so tag objects will contain _other_ tag objects that were under it in the html tree.\n",
        "\n",
        "- **Each tag has a:**\n",
        "    - `.name`\n",
        "    - `.contents`\n",
        "    - `.string`\n",
        "    \n",
        "- **A tag can be access by name (like a column in a dataframe using dot notation)**\n",
        "    - and then you can access the tags within the new tag-variable just like the first tag\n",
        "    ```python\n",
        "    # Access tags by name\n",
        "    meta = soup.meta\n",
        "    head = soup.head\n",
        "    body = soup.body\n",
        "    # and so on...\n",
        "    ```\n",
        "- [!] ***BUT this will only return the FIRST tag of that type, to access all occurances of a tag-type, we will need to navigate the html family tree***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZCLylw9RkVL"
      },
      "source": [
        "\n",
        "## Navigating the HTML Family Tree: Children, siblings, and parents\n",
        "\n",
        "- **Each tag is located within a tree-hierarchy of parents, siblings, and children**\n",
        "    - The family-relation is based on the identation level of the tags.\n",
        "\n",
        "- **Methods/attributes for the location/related tags of a tag**\n",
        "    - `.parent`, `.parents`\n",
        "    - `.child`, `.children`\n",
        "    - `.descendents`\n",
        "    - `.next_sibling`, `.previous_sibling`\n",
        "\n",
        "- *Note: a newline character `\\n` is also considered a tag/sibling/child*\n",
        "\n",
        "#### Accessing Child Tags\n",
        "\n",
        "- To get to later occurances of a tag type (i.e. the 2nd `<p>` tag in a tree), we need to navigate through the parent tag's `children`\n",
        "    - To access an iterable list of a tag's children use `.children`\n",
        "        - But, this only returns its *direct children*  (one indentation level down)     \n",
        "        \n",
        "    ```python\n",
        "    # print direct children of the body tag\n",
        "    body = soup.body\n",
        "    for child in body.children:\n",
        "        # print child if its not empty\n",
        "        print(child if child is not None else ' ', '\\n\\n')  # '\\n\\n' for visual separation\n",
        "    ```\n",
        "- To access *all children* use `.descendents`\n",
        "    - Returns all chidren and children of children\n",
        "    ```python\n",
        "    for child in body.descendents:\n",
        "        # print all children/grandchildren, etc\n",
        "        print(child if child is not None else ' ','\\n\\n')  \n",
        "    ```\n",
        "    \n",
        "#### Accessing Parent tags\n",
        "\n",
        "- To access the parent of a tag use `.parent`\n",
        "```python\n",
        "title = soup.head.title\n",
        "print(title.parent.name)\n",
        "```\n",
        "\n",
        "- To get a list of _all parents_ use `.parents`\n",
        "```python\n",
        "title = soup.head.title\n",
        "for parent in title.parents:\n",
        "    print(parent.name)\n",
        "```\n",
        "\n",
        "#### Accessing Sibling tags\n",
        "- siblings are tags in the same tree indentation level\n",
        "- `.next_sibling`, `.previous_sibling`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXktQsDvWW0A"
      },
      "source": [
        "## Searching Through Soup\n",
        "\n",
        "\n",
        "### Finding the target tags to isolate\n",
        "Using example  from  [Wikipedia article](https://en.wikipedia.org/wiki/Stock_market)\n",
        "where we are trying to isolate the body of the article content.\n",
        "\n",
        "\n",
        "- **Examine the website using Chrome's inspect view.**\n",
        "\n",
        "    - Press F12 or right-click > inspect\n",
        "\n",
        "    - Use the mouse selector tool (top left button) to explore the web page content for your desired target\n",
        "        - the web page element will be highlighted on the page itself and its corresponding entry in the document tree.\n",
        "        - Note: click on the web page with the selector in order to keep it selected in the document tree\n",
        "\n",
        "    - Take note of any identifying attributes for the target tag (class, id, etc)\n",
        "<img src=\"https://drive.google.com/uc?export-download&id=1KifQ_ukuXFdnCh1Tz1rwzA_cWkB_45mf\" width=450>\n",
        "\n",
        "### Using BeautifulSoup's search functions\n",
        "Note: while the process below is a decent summary, there is more nuance to html/css tags than I personally have been able to digest.\n",
        "    - If something doesn't work as expected/explained, please verify in the documentation.\n",
        "        - [BeauitfulSoup documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#beautiful-soup-documentation)\n",
        "        - [docs for .find_all()](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#find-all)\n",
        "    \n",
        "- **BeautifulSoup has methods for searching through descendent-tags**\n",
        "    - `.find`\n",
        "    - `.find_all`\n",
        "    \n",
        "- **Using `.find_all()`**\n",
        "    - Searches through all descendent tags and returns a result set (list of tag objects)\n",
        "```python\n",
        "# How to get results from .find_all()\n",
        "results = soup.find_all(name, attrs, recursive, string, limit,**kwargs) `\n",
        "```        \n",
        "    - `.find_all()` parameters:\n",
        "        - `name` _(type of tags to consider)_\n",
        "            - only consider tags with this name\n",
        "                - Ex: 'a',  'div', 'p' ,etc.\n",
        "        - `atrrs`_(css attributes that you are looking for in your target tag)_\n",
        "            - enter an attribute such as the class or id as a string\n",
        "\n",
        "                `attrs='mw-content-ltr'`\n",
        "            - if passing more than one attribute, must use a dictionary:\n",
        "\n",
        "            `attrs={'class':'mw-content-ltr', 'id':'mw-content-text'}`\n",
        "        - `recursive`_(Default=True)_\n",
        "            - search all children (`True`)\n",
        "            - search only  direct children(`False`)\n",
        "\n",
        "        - `string`\n",
        "            - search for text _inside_ of tags instead of the tags themselves\n",
        "            - can be regular expression\n",
        "        - `limit`\n",
        "            - How many results you want it to return\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "jJFr-RC7vzI9",
        "outputId": "4d0527cd-1220-466a-e08e-06a76259e94b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fake_useragent\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/79/af647635d6968e2deb57a208d309f6069d31cb138066d7e821e575112a80/fake-useragent-0.1.11.tar.gz\n",
            "Building wheels for collected packages: fake-useragent\n",
            "  Building wheel for fake-useragent (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/63/09/d1dc15179f175357d3f5c00cbffbac37f9e8690d80545143ff\n",
            "Successfully built fake-useragent\n",
            "Installing collected packages: fake-useragent\n",
            "Successfully installed fake-useragent-0.1.11\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (4.2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install fake_useragent\n",
        "!pip install lxml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUuCKNCdbYdS"
      },
      "source": [
        "# 2) Walk-through example/code\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwhquyO3x9q7"
      },
      "source": [
        "    - James functions\n",
        "    - Functional code scraping wikipedia pages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ng-HN_rRymjI"
      },
      "source": [
        "## Walkthrough - using James' functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "1Pg3VWWmypij",
        "outputId": "1b76df53-a624-4173-8269-f96029873080"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/wiki/Rent_seeking', '/wiki/Rhine_capitalism', '/wiki/State-sponsored_capitalism', '/wiki/Global_capitalism', '/wiki/Perspectives_on_capitalism']\n",
            "['https://en.wikipedia.org/wiki/Rent_seeking', 'https://en.wikipedia.org/wiki/Rhine_capitalism', 'https://en.wikipedia.org/wiki/State-sponsored_capitalism', 'https://en.wikipedia.org/wiki/Global_capitalism', 'https://en.wikipedia.org/wiki/Perspectives_on_capitalism']\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse, urljoin\n",
        "\n",
        "from fake_useragent import UserAgent\n",
        "url = 'https://en.wikipedia.org/wiki/Stock_market'\n",
        "soup = cook_soup_from_url(url,sleep_time=1)\n",
        "\n",
        "\n",
        "## Get all links that match are interal wikipedia redirects [yes?]\n",
        "kwds = {'class':'mw-redirect'}\n",
        "links = get_all_links(soup)#,kwds)\n",
        "\n",
        "\n",
        "# preview first 5 links\n",
        "print(links[:5])\n",
        "\n",
        "\n",
        "# Turn relative links into absolute links\n",
        "abs_links = make_absolute_links(url,links)\n",
        "print(abs_links[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "id": "6tCwXLiN7UCk",
        "outputId": "d360207c-aa0f-40eb-e8f3-de1d0d90b6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of input links: == # of soups in batch:\n",
            "5 == 5\n",
            "\n",
            "Each soup_dict has  dict_keys(['_url', 'path', 'soup'])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "bs4.BeautifulSoup"
            ]
          },
          "execution_count": 118,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Selecting only the first 5 links to test\n",
        "abs_links_for_soups = abs_links[:5]\n",
        "\n",
        "\n",
        "# Cooking a batch of soups from those chosen links\n",
        "batch_of_soups = cook_batch_of_soups(abs_links_for_soups, sleep_time=2)\n",
        "\n",
        "# batch_of_soups is a list as long as the input link_list\n",
        "print(f'# of input links: == # of soups in batch:\\n{len(abs_links_for_soups)} == {len(batch_of_soups)}\\n')\n",
        "\n",
        "# batch_of_soups is a list of soup-dictionaries\n",
        "soup_dict = batch_of_soups[0]\n",
        "print('Each soup_dict has ',soup_dict.keys())\n",
        "\n",
        "# the page's soup is stored under soup_dict['soup']\n",
        "soup_from_soup_dict = soup_dict['soup']\n",
        "type(soup_from_soup_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFVnj20YmQK2"
      },
      "source": [
        "#### Notes on extracting content.\n",
        "- Edit the `extract_target_text function` in the James' functions settings or uncomment and use the `extract_target_text_custom function` below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "QwU8aPHJhAVm",
        "outputId": "034b20bf-67f1-4f4f-8450-33ee0f4ab111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " Rent-seeking is a concept in public choice theory as well as in economics, that involves seeking to increase one's share of existing wealth without creating new wealth. Rent-seeking results in reduced economic efficiency through misallocation of resources, reduced wealth-creation, lost government revenue, heightened income inequality,[1] and potential national decline.\n",
            " Attempts at capture of regulatory agencies to gain a coercive monopoly can result in advantages for the rent seeker in a market while imposing disadvantages on their incorrupt competitors. This is one of many possible forms of rent-seeking behavior.\n",
            " The idea of rent-seeking was developed by Gordon Tullock in 1967,[2] while the expression rent-seeking itself was coined in 1974 by Anne Krueger.[3] The word \"rent\" does not refer specifically to payment on a lease but rather to Adam Smith's division of incomes into profit, wage, and rent.[4] The origin of the term refers to gaining control of land or other natural resour\n"
          ]
        }
      ],
      "source": [
        "## ADDING extract_target_text to precisely target text\n",
        "# def extract_target_text_custom(soup_or_tag,tag_name='p', attrs_dict=None, join_text =True, save_files=False):\n",
        "#     \"\"\"User-specified function to add extraction of specific content during 'cook batch of soups'\"\"\"\n",
        "\n",
        "#     if attrs_dict==None:\n",
        "#         found_tags = soup_or_tag.find_all(name=tag_name)\n",
        "#     else:\n",
        "#         found_tags = soup_or_tag.find_all(name=tag_name,attrs=attrs_dict)\n",
        "\n",
        "\n",
        "#     # if extracting from multiple tags\n",
        "#     output=[]\n",
        "#     output = [tag.text for tag in found_tags if tag.text is not None]\n",
        "\n",
        "#     if join_text == True:\n",
        "#         output = ' '.join(output)\n",
        "\n",
        "#     ## ADDING SAVING EACH\n",
        "#     if save_files==True:\n",
        "#         text = output #soup.body.string\n",
        "#         filename =f\"drive/My Drive/text_extract_{url_dict_key}.txt\"\n",
        "#         soup_dict['filename'] = filename\n",
        "#         with open(filename,'w+') as f:\n",
        "#             f.write(text)\n",
        "#         print(f'File  successfully saved as {filename}')\n",
        "\n",
        "#     return  output\n",
        "\n",
        "# ####################\n",
        "\n",
        "## RUN A LOOP TO ADD EXTRACTED TEXT TO EACH SOUP IN THE BATCH\n",
        "for i, soup_dict in enumerate(batch_of_soups):\n",
        "\n",
        "    # Get the soup from the dict\n",
        "    soup = soup_dict['soup']\n",
        "\n",
        "    # Extract text\n",
        "    extracted_text = extract_target_text(soup)\n",
        "\n",
        "    # Add key:value for results of extract\n",
        "    soup_dict['extracted'] = extracted_text\n",
        "\n",
        "    # Replace the old soup_dict with the new one with 'extracted'\n",
        "    batch_of_soups[i] = soup_dict\n",
        "\n",
        "example_extracted_text=batch_of_soups[0]['extracted']\n",
        "print(example_extracted_text[:1000])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Web Scraping 101_share.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
